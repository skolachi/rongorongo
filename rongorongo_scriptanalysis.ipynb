{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rongorongo-scriptanalysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMb86NR++44B06oShv1eXQG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skolachi/rongorongo/blob/master/rongorongo_scriptanalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8pOb5y6ikPV",
        "outputId": "8f91a4e5-bedf-45d9-f65a-bd898b8cf036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-03 20:16:18--  http://kohaumotu.org/rongorongo_org/concord/concord1.zip\n",
            "Resolving kohaumotu.org (kohaumotu.org)... 162.215.248.227\n",
            "Connecting to kohaumotu.org (kohaumotu.org)|162.215.248.227|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 250143 (244K) [application/zip]\n",
            "Saving to: ‘concord1.zip’\n",
            "\n",
            "concord1.zip        100%[===================>] 244.28K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-04-03 20:16:20 (2.74 MB/s) - ‘concord1.zip’ saved [250143/250143]\n",
            "\n",
            "--2022-04-03 20:16:20--  http://kohaumotu.org/rongorongo_org/concord/concord2.zip\n",
            "Resolving kohaumotu.org (kohaumotu.org)... 162.215.248.227\n",
            "Connecting to kohaumotu.org (kohaumotu.org)|162.215.248.227|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 229017 (224K) [application/zip]\n",
            "Saving to: ‘concord2.zip’\n",
            "\n",
            "concord2.zip        100%[===================>] 223.65K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-04-03 20:16:20 (2.51 MB/s) - ‘concord2.zip’ saved [229017/229017]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://kohaumotu.org/rongorongo_org/concord/concord1.zip\n",
        "!wget http://kohaumotu.org/rongorongo_org/concord/concord2.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir rongorongo\n",
        "!unzip concord1.zip -d rongorongo/ \n",
        "!unzip concord2.zip -d rongorongo/\n",
        "!rm concord1.zip concord2.zip\n",
        "!cat rongorongo/*.CCD >> rongorongo/fullconcordance.CCD\n",
        "!wc -l rongorongo/fullconcordance.CCD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYpw5gn6ltNH",
        "outputId": "41e66bd5-4e50-4693-860a-c703738ec1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  concord1.zip\n",
            "  exploding: rongorongo/001.CCD      \n",
            "  exploding: rongorongo/005.CCD      \n",
            "  exploding: rongorongo/010.CCD      \n",
            "  exploding: rongorongo/025.CCD      \n",
            "  exploding: rongorongo/050.CCD      \n",
            "  exploding: rongorongo/070.CCD      \n",
            "Archive:  concord2.zip\n",
            "  exploding: rongorongo/080.CCD      \n",
            "  exploding: rongorongo/100.CCD      \n",
            "  exploding: rongorongo/200.CCD      \n",
            "  exploding: rongorongo/300.CCD      \n",
            "  exploding: rongorongo/400.CCD      \n",
            "  exploding: rongorongo/500.CCD      \n",
            "  exploding: rongorongo/600.CCD      \n",
            "  exploding: rongorongo/700.CCD      \n",
            "16568 rongorongo/fullconcordance.CCD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def read_corpus(corpusfile):\n",
        "  return re.findall(r'[A-Z]{1}[a-z]{1}[0-9]{2}\\.[0-9]{3}\\:([^\\n]*)',open(corpusfile).read())"
      ],
      "metadata": {
        "id": "7uKTILtwpdVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = read_corpus('rongorongo/fullconcordance.CCD')\n",
        "len(corpus)\n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUO7TWfrp2dv",
        "outputId": "78b80530-58a0-4371-d09d-558aea19ff71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['200j.52x-200-8!-200.|1!*(2-3)-48!-1!.10x!-4!.64a-202-280-191b-5-',\n",
              " '1.10-5j.52-22by.243-|1t-(1-3)*0!-5j.6-7-93-50b.74x-8-400!-281-1.',\n",
              " ' 28x.95j-73af-1-380.|1-(5-8)*(3-5)-421d-95d-87.62-200.700.61-4.',\n",
              " ' 100.62-16.62x-202s-|1-0!-0!.10-59f-35.59f.55b-35-460-10.55b-7-',\n",
              " '700?-755-90-200-200.|1?-0!*0!-93-739-670-0!-70?-700x?-28-85?-1?-',\n",
              " ' 510.7-10.1.11-1-99-|1-0!*0!-200?.200.10f.55b-73f-49f-69-200-6-',\n",
              " ' 3-37?-5-306?.37?-5-|1-0!*0!-206?-7-306.3-28.95-642-43:1?-65V-',\n",
              " '280?-1-280?-1?-280?-|1?-0!-1.6-522-1.62-600-62.1-522-6.1-380.1.',\n",
              " '6.20.10-50-71.76-11-|1V-0!-5-6.74f-596.76-726.77?-400V-70.76-20.',\n",
              " '9-8!-1a?-114a.9!-0!-|1!-0!-8!*']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#punctuation = ['-','.',':','\\'','*']\n",
        "samp = ''\n",
        "def line2sign(line):\n",
        "  signs = []\n",
        "  line = re.sub('\\([0-9]{1}\\-[0-9]{1}\\)','-000-',line)\n",
        "  for c in re.split(r'[\\-\\.\\:\\'\\*]',line):\n",
        "    if c != '':\n",
        "      sign = re.sub('[^0-9]','',c)\n",
        "      signs.append('0'*(3-len(sign))+sign)\n",
        "  \n",
        "  return signs"
      ],
      "metadata": {
        "id": "xyUf7CBHuaxa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sign_sequences.txt','w') as f:\n",
        "  for text in corpus:\n",
        "    f.write('{}\\n'.format(' '.join(line2sign(text))))"
      ],
      "metadata": {
        "id": "GK7HaDzR7Qmf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIEGtaW6KGnX",
        "outputId": "1ae1e6cb-4793-4180-8521-ea2161619fb2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 39.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from tokenizers import Tokenizer\n",
        "from transformers import BertTokenizerFast\n",
        "from tokenizers.models import Unigram, WordLevel, WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import decoders"
      ],
      "metadata": {
        "id": "oDU1MHV0LWAT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer():\n",
        "  dictfiles = ['sign_sequences.txt']\n",
        "\n",
        "  tokenizer = BertWordPieceTokenizer()\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  tokenizer.decoder = decoders.WordPiece()\n",
        "\n",
        "  tokenizer.train(files=dictfiles,min_frequency=2,special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"])\n",
        "  if not os.path.isdir(\"rongorongoLM\"):\n",
        "    os.mkdir(\"rongorongoLM\")\n",
        "  tokenizer.save_model(\"rongorongoLM\")"
      ],
      "metadata": {
        "id": "gm9p1IQMLwJ-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenizer()\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"rongorongoLM\")"
      ],
      "metadata": {
        "id": "qG9pf59zMMet"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "  tokenizer=tokenizer,\n",
        "  file_path=\"./sign_sequences.txt\",\n",
        "  block_size=128,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8HryRmtMnSb",
        "outputId": "ca3b6bac-acda-4ea5-db4e-6fb643b198ac"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig\n",
        "\n",
        "config = BertConfig(\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=6,\n",
        "    num_hidden_layers=3,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "metadata": {
        "id": "-uCf7W8zNKfu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM\n",
        "\n",
        "model = BertForMaskedLM(config=config)\n",
        "\n",
        "print(\"Number of model parameters: \",model.num_parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7UMfb6ANR5b",
        "outputId": "621b7bb8-3fe2-43bc-f2b1-ae80932de555"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters:  45724218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.5\n",
        ")"
      ],
      "metadata": {
        "id": "-mpOjO_uNhZG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def train_lm():\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./rongorongoLM\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=1,\n",
        "        per_gpu_train_batch_size=128,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        prediction_loss_only=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(\"./rongorongoLM\")"
      ],
      "metadata": {
        "id": "KKux84CiORRN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "GrCgBDnZOips",
        "outputId": "8feafc7f-f142-454a-a648-2d4c9b5bd166"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 14653\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 115\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [115/115 27:24, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to ./rongorongoLM\n",
            "Configuration saved in ./rongorongoLM/config.json\n",
            "Model weights saved in ./rongorongoLM/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "  \"fill-mask\",\n",
        "  model=\"./rongorongoLM\",\n",
        "  tokenizer=\"./rongorongoLM\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXSsGpzYWnaQ",
        "outputId": "acbb631b-3156-4f76-86f4-393a5bbde512"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file ./rongorongoLM/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"./rongorongoLM\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 6,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file ./rongorongoLM/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"./rongorongoLM\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 6,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file ./rongorongoLM/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
            "\n",
            "All the weights of BertForMaskedLM were initialized from the model checkpoint at ./rongorongoLM.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file ./rongorongoLM/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"./rongorongoLM\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 6,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Didn't find file ./rongorongoLM/tokenizer.json. We won't load it.\n",
            "Didn't find file ./rongorongoLM/added_tokens.json. We won't load it.\n",
            "Didn't find file ./rongorongoLM/special_tokens_map.json. We won't load it.\n",
            "Didn't find file ./rongorongoLM/tokenizer_config.json. We won't load it.\n",
            "loading file ./rongorongoLM/vocab.txt\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n",
            "loading configuration file ./rongorongoLM/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"./rongorongoLM\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 6,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file ./rongorongoLM/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"./rongorongoLM\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 6,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[10], ' '.join(line2sign(corpus[10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GcU5-FrW-u-",
        "outputId": "76142c95-5e4b-4432-a0fd-8d69f23fe830"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('4-0!-0!-4-99?-0!-0!-|1?-0!-14?-8-600?-1?*385-1*0!-59f-22a-22a*8-',\n",
              " '004 000 000 004 099 000 000 001 000 014 008 600 001 385 001 000 059 022 022 008')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"004 000 000 004 [MASK] 000 000 001 000 [MASK] 008 600 001 385 001 000 059 022 022 008\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGo57iokXUJx",
        "outputId": "1caa9c6a-15e8-455c-d69b-63da6dfab0ba"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'score': 0.07114849984645844,\n",
              "   'sequence': '[CLS] 004 000 000 004 001 000 000 001 000 [MASK] 008 600 001 385 001 000 059 022 022 008 [SEP]',\n",
              "   'token': 28,\n",
              "   'token_str': '001'},\n",
              "  {'score': 0.020147668197751045,\n",
              "   'sequence': '[CLS] 004 000 000 004 004 000 000 001 000 [MASK] 008 600 001 385 001 000 059 022 022 008 [SEP]',\n",
              "   'token': 34,\n",
              "   'token_str': '004'},\n",
              "  {'score': 0.0186594408005476,\n",
              "   'sequence': '[CLS] 004 000 000 004 003 000 000 001 000 [MASK] 008 600 001 385 001 000 059 022 022 008 [SEP]',\n",
              "   'token': 37,\n",
              "   'token_str': '003'},\n",
              "  {'score': 0.01518901064991951,\n",
              "   'sequence': '[CLS] 004 000 000 004 076 000 000 001 000 [MASK] 008 600 001 385 001 000 059 022 022 008 [SEP]',\n",
              "   'token': 31,\n",
              "   'token_str': '076'},\n",
              "  {'score': 0.013096786104142666,\n",
              "   'sequence': '[CLS] 004 000 000 004 000 000 000 001 000 [MASK] 008 600 001 385 001 000 059 022 022 008 [SEP]',\n",
              "   'token': 39,\n",
              "   'token_str': '000'}],\n",
              " [{'score': 0.06870299577713013,\n",
              "   'sequence': '[CLS] 004 000 000 004 [MASK] 000 000 001 000 001 008 600 001 385 001 000 059 022 022 008 [SEP]',\n",
              "   'token': 28,\n",
              "   'token_str': '001'},\n",
              "  {'score': 0.02042429707944393,\n",
              "   'sequence': '[CLS] 004 000 000 004 [MASK] 000 000 001 000 003 008 600 001 385 001 000 059 022 022 008 [SEP]',\n",
              "   'token': 37,\n",
              "   'token_str': '003'},\n",
              "  {'score': 0.02037578821182251,\n",
              "   'sequence': '[CLS] 004 000 000 004 [MASK] 000 000 001 000 004 008 600 001 385 001 000 059 022 022 008 [SEP]',\n",
              "   'token': 34,\n",
              "   'token_str': '004'},\n",
              "  {'score': 0.015000039711594582,\n",
              "   'sequence': '[CLS] 004 000 000 004 [MASK] 000 000 001 000 076 008 600 001 385 001 000 059 022 022 008 [SEP]',\n",
              "   'token': 31,\n",
              "   'token_str': '076'},\n",
              "  {'score': 0.013963338918983936,\n",
              "   'sequence': '[CLS] 004 000 000 004 [MASK] 000 000 001 000 006 008 600 001 385 001 000 059 022 022 008 [SEP]',\n",
              "   'token': 38,\n",
              "   'token_str': '006'}]]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}